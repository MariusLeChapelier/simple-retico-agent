from llama_cpp import Llama

my_model = my_model = Llama(
            model_path="./models/mistral-7b-instruct-v0.2.Q4_K_S.gguf", 
            n_ctx=300, 
            n_gpu_layers=100,
            device_map="cuda",
        )
# my_prompt = "<s> [INST] Hi [/INST] Hi</s> [INST] Hi [/INST]"
my_prompt = "<<SYS>> you are a teacher. I am a child. <</SYS>> [INST] Hi. Hello. Teach me mathematics. [/INST]"
# my_prompt = "<<SYS>>  This is a spoken dialog scenario between a teacher and a 8 years old child student. \
#         The teacher is teaching mathemathics to the child student. \
#         As the student is a child, the teacher needs to stay gentle all the time. Please provide the next valid response for the followig conversation.\
#         You play the role of a teacher. Here is the beginning of the conversation : <</SYS>> \
#         [INST] Child : Hello ! \
#         Teacher : Hi! How are your today ? \
#         Child : I am fine, and I can't wait to learn mathematics ! \
#         Teacher : Okay, we will start with the simpler operation in mathematics, addition. What do you know about addition ? \
#         Child : Humm, I know that one plus one equals two ! \
#         Teacher : That's right ! And this is a good start. So, addition is the operation of adding two numbers together to get a bigger number that is their sum. In our case, we added one to one to get the bigger number two, that is their sum. Is it clear ? \
#         Child : I... I am not sure...  \
#         Teacher : Okay I will take another example, when we add one number, let's say two, to another number, let's say one, we get as a result three, a number bigger than both our previous numbers. \
#         Child : okay, I think I get it now ! [/INST]"
tokens = my_model.tokenize(bytes(my_prompt, "utf-8"), special=True)
last_sentence = b""

print("EOS = ", my_model.token_eos())

def stop(tokens, logits):
     return tokens[-1] == my_model.token_eos()

# def stop_criteria(tokens, logits):
#         """
#         Function used by the LLM to stop generate tokens when it meets certain criteria.

#         Args:
#             tokens (_type_): tokens generated by the LLM
#             logits (_type_): _description_

#         Returns:
#             bool: returns True if it generated one of the tokens corresponding to STOP_TOKEN_IDS or STOP_TOKEN_TEXT.
#         """

#         is_stopping_id = tokens[-1] in STOP_TOKEN_IDS
#         is_stopping_text = my_model.detokenize([tokens[-1]]) in STOP_TOKEN_TEXT
#         return is_stopping_id or is_stopping_text

for token in my_model.generate(
    tokens,
    stopping_criteria=stop
    ): 
    a = my_model.detokenize([token])       
    last_sentence += a
    # print(a.decode("utf-8"))
    # print(last_sentence.decode("utf-8"))
    if token == 2:
        print("EOS")

print(last_sentence)
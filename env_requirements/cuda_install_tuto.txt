# clone repo
`git clone https://github.com/articulab/simple-retico-agent`

# Create your virtual environment with python 3.11.7 version 

With conda : 
`conda env create -n [env_name] python=3.11.7`

# install simple-retico-agent package and its dependencies
`pip install .`

# install cuda support for Deep Learning retico modules (ASR, LLM, TTS) (to speed up greatly the system's execution)
modify this line with your installed cuda toolkit version (I put `cu118` because I am using cuda 11.8 version)
`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --force-reinstall --no-cache`

# installation for llama-cpp-python's cuda support (GPU execution)
`pip uninstall llama-cpp-python && set "CMAKE_ARGS=-DGGML_CUDA=on" && set "FORCE_CMAKE=1" && pip install llama-cpp-python --no-cache-dir`

if it doesn't work, and you are using conda, you can copy the exact environement I am using, and try to reinstall llama-cpp-python's cuda supported version: 
`conda env update -n [env_name] -f env_requirements/retico_cuda_curr.yml --prune`
`pip uninstall llama-cpp-python && set "CMAKE_ARGS=-DGGML_CUDA=on" && set "FORCE_CMAKE=1" && pip install llama-cpp-python --no-cache-dir`


create conda env with python 3.11.7
- `conda create -n env python=3.11.7`
install pyaudio (as it is not part of retico-core anymore)
- `pip install pyaudio`
install all dependencies (highest possible version)
- `pip install .` (all dependencies in pyproject)
-> System working for cpu : YES
install cuda and C related libs for CUDA support (TTS, ASR, LLM)
- `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --force-reinstall --no-cache`
-> raises following error (more a warning): 
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.3 which is incompatible.
pydantic 2.10.1 requires typing-extensions>=4.12.2, but you have typing-extensions 4.9.0 which is incompatible.
thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= "3.9", but you have numpy 1.26.3 which is incompatible.
typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.9.0 which is incompatible.
Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 pillow-10.2.0 sympy-1.13.1 torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118 typing-extensions-4.9.0
```
-> System (but LLM) working on CUDA : YES
reinstall llama-cpp-python with CUDA support with :
- `pip uninstall llama-cpp-python && set "CMAKE_ARGS=-DGGML_CUDA=on" && set "FORCE_CMAKE=1" && pip install --no-cache-dir llama-cpp-python`
-> LLM on CUDA working ?